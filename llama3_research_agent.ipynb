{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c707a92-6f82-44d8-8de0-fa612064df5e",
   "metadata": {},
   "source": [
    "# Local Web Research Agent w/ Llama 3 8b\n",
    "\n",
    "### [Llama 3 Release](https://llama.meta.com/llama3/)\n",
    "\n",
    "### [Ollama Llama 3 Model](https://ollama.com/library/llama3)\n",
    "---\n",
    "\n",
    "![diagram](local_agent_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ba1023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715059b3-857c-456d-a740-24e2551d739d",
   "metadata": {},
   "source": [
    "---\n",
    "[Llama 3 Prompt Format](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)\n",
    "\n",
    "### Special Tokens used with Meta Llama 3\n",
    "* **<|begin_of_text|>**: This is equivalent to the BOS token\n",
    "* **<|eot_id|>**: This signifies the end of the message in a turn.\n",
    "* **<|start_header_id|>{role}<|end_header_id|>**: These tokens enclose the role for a particular message. The possible roles can be: system, user, assistant.\n",
    "* **<|end_of_text|>**: This is equivalent to the EOS token. On generating this token, Llama 3 will cease to generate more tokens.\n",
    "A prompt should contain a single system message, can contain multiple alternating user and assistant messages, and always ends with the last user message followed by the assistant header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f2cb84-6abf-4a6c-8d1f-cdc6474b77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying final output format\n",
    "from IPython.display import display, Markdown, Latex\n",
    "# LangChain Dependencies\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langgraph.graph import END, StateGraph\n",
    "# For State Graph \n",
    "from typing_extensions import TypedDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d39b8539-1bfe-4001-b7b2-6752a77846d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"L3 Research Agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b341d1d-0a59-4c03-8558-759ea00171bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining LLM\n",
    "local_llm = 'llama3'\n",
    "llama3 = ChatOllama(model=local_llm, temperature=0)\n",
    "llama3_json = ChatOllama(model=local_llm, format='json', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7813ac-791f-4035-a5ec-04810d5de5f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Web Search Tool\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(max_results=25)\n",
    "web_search_tool = DuckDuckGoSearchRun(api_wrapper=wrapper)\n",
    "\n",
    "# Test Run\n",
    "# resp = web_search_tool.invoke(\"home depot news\")\n",
    "# resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d798a81-6ed6-4a4f-a1d9-93b4e3059fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Prompt\n",
    "\n",
    "generate_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    \n",
    "    You are an AI assistant for Research Question Tasks, that synthesizes web search results. \n",
    "    Strictly use the following pieces of web search context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    keep the answer concise, but provide all of the details you can in the form of a research report. \n",
    "    Only make direct references to material if provided in the context.\n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    \n",
    "    Question: {question} \n",
    "    Web Search Context: {context} \n",
    "    Answer: \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "generate_chain = generate_prompt | llama3 | StrOutputParser()\n",
    "\n",
    "# Test Run\n",
    "# question = \"How are you?\"\n",
    "# context = \"\"\n",
    "# generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "# print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49fa1965-6bd4-4dfc-9eb8-96c6cff7b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choice': 'generate'}\n"
     ]
    }
   ],
   "source": [
    "# Router\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    \n",
    "    You are an expert at routing a user question to either the generation stage or web search. \n",
    "    Use the web search for questions that require more context for a better answer, or recent events.\n",
    "    Otherwise, you can skip and go straight to the generation phase to respond.\n",
    "    You do not need to be stringent with the keywords in the question related to these topics.\n",
    "    Give a binary choice 'web_search' or 'generate' based on the question. \n",
    "    Return the JSON with a single key 'choice' with no premable or explanation. \n",
    "    \n",
    "    Question to route: {question} \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "question_router = router_prompt | llama3_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "question = \"What's up?\"\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ab4128-e0b0-4f49-9f36-1d3bf5636715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Macom recent news'}\n"
     ]
    }
   ],
   "source": [
    "# Query Transformation\n",
    "\n",
    "query_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    <|begin_of_text|>\n",
    "    \n",
    "    <|start_header_id|>system<|end_header_id|> \n",
    "    \n",
    "    You are an expert at crafting web search queries for research questions.\n",
    "    More often than not, a user will ask a basic question that they wish to learn more about, however it might not be in the best format. \n",
    "    Reword their query to be the most effective web search string possible.\n",
    "    Return the JSON with a single key 'query' with no premable or explanation. \n",
    "    \n",
    "    Question to transform: {question} \n",
    "    \n",
    "    <|eot_id|>\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "query_chain = query_prompt | llama3_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "question = \"What's happened recently with Macom?\"\n",
    "print(query_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c8e922-3f00-48d6-83cb-cc78a2292838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search_query: revised question for web search\n",
    "        context: web_search result\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    search_query : str\n",
    "    context : str\n",
    "\n",
    "# Node - Generate\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Generating Final Response\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Answer Generation\n",
    "    generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "# Node - Query Transformation\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform user question to web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended search query\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Optimizing Query for Web Search\")\n",
    "    question = state['question']\n",
    "    gen_query = query_chain.invoke({\"question\": question})\n",
    "    search_query = gen_query[\"query\"]\n",
    "    return {\"search_query\": search_query}\n",
    "\n",
    "\n",
    "# Node - Web Search\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to context\n",
    "    \"\"\"\n",
    "\n",
    "    search_query = state['search_query']\n",
    "    print(f'Step: Searching the Web for: \"{search_query}\"')\n",
    "    \n",
    "    # Web search tool call\n",
    "    search_result = web_search_tool.invoke(search_query)\n",
    "    return {\"context\": search_result}\n",
    "\n",
    "\n",
    "# Conditional Edge, Routing\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    route question to web search or generation.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Step: Routing Query\")\n",
    "    question = state['question']\n",
    "    output = question_router.invoke({\"question\": question})\n",
    "    if output['choice'] == \"web_search\":\n",
    "        print(\"Step: Routing Query to Web Search\")\n",
    "        return \"websearch\"\n",
    "    elif output['choice'] == 'generate':\n",
    "        print(\"Step: Routing Query to Generation\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f665713-e80b-4d86-8015-77ba55506004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the nodes\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Build the edges\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"websearch\")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "local_agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f53aa05-20b2-420e-9a8f-bf12b1e547ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query):\n",
    "    output = local_agent.invoke({\"question\": query})\n",
    "    print(\"=======\")\n",
    "    display(Markdown(output[\"generation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a1b135d-131e-4276-b40c-12ea8b78c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n",
      "Step: Routing Query to Web Search\n",
      "Step: Optimizing Query for Web Search\n",
      "Step: Searching the Web for: \"Macom recent news\"\n",
      "Step: Generating Final Response\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided web search context, here is what's been up with MACOM recently:\n",
       "\n",
       "* On December 2, 2023, MACOM Technology Solutions Holdings, Inc. (\"MACOM\") (NASDAQ: MTSI) completed its acquisition of the radio frequency business (the \"RF Business\") of Wolfspeed, Inc.\n",
       "* In August 2023, MACOM announced that it had entered into a definitive agreement to acquire the RF Business of Wolfspeed, Inc.\n",
       "* On February 1, 2024, MACOM announced its financial results for its fiscal first quarter ended December 29, 2023. Revenue was $157.1 million, a decrease of 12.7%, compared to $180.1 million in the previous year.\n",
       "* In January 2024, MACOM plans to announce financial results for its first quarter fiscal year 2024 ended December 29, 2023, before market open on Thursday, February 1, 2024.\n",
       "* On May 2, 2024, MACOM announced its financial results for its fiscal second quarter ended March 29, 2024. Revenue was $181.2 million, an increase of 7.0%, compared to $169.4 million in the previous year.\n",
       "* In November 2023, MACOM Technology Solutions Holdings, Inc. (\"MACOM\") (Nasdaq: MTSI), a leading supplier of semiconductor products, announced its financial results for its fiscal fourth quarter and fiscal year ended September 29, 2023.\n",
       "\n",
       "Additionally, there have been some local news stories about Macomb County, Michigan, including:\n",
       "\n",
       "* A major road construction project began on June 10, 2024, on a stretch of M-53 in northern Macomb County.\n",
       "* On March 16, 2024, officers with the Macomb Police Department responded to an active domestic violence and officer-involved shooting case at an apartment complex in Macomb.\n",
       "* In November 2023, a series of schools in Macomb County received bomb threats over the weekend, causing at least one school to cancel classes.\n",
       "\n",
       "Please note that these are just general updates and may not be directly related to MACOM's business or financial performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test it out!\n",
    "run_agent(\"What's been up with Macom recently?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2eed6-c12e-4c8e-8e20-f8febbc0d0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
